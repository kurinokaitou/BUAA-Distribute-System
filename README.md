# 展示讲稿

1. 分析NTP的PDU

其中我们主要用到的部分为参考时钟（Stratum），最大间隔（Poll），精度（Precision），初始时间戳（t0)、接受时间戳(t1)和发送时间戳(t2)，以及从本地获取的时间(t3)。

**Stratum**这显示了来源的层，如其最近收到的样本中所报告的那样。层1表示一台具有本地连接的参考时钟的计算机。与第1层计算机同步的计算机位于第2层。与第2层计算机同步的计算机位于第3层，依此类推。

**Poll**这显示轮询源的速率，以秒为单位的时间间隔的以2为底的对数。因此，值为6表示每64秒进行一次测量。

**Precision**代表了这个授权时间源的精度，以2位低precision为指数，如-6则代表精度为1/64秒

![](https://upload.wikimedia.org/wikipedia/commons/8/8d/NTP-Algorithm.svg)

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/9a9ff4428ee0102d2c62b260df1b38b26989b383)

![{\displaystyle \delta ={(t_{3}-t_{0})-(t_{2}-t_{1})},}](https://wikimedia.org/api/rest_v1/media/math/render/svg/65338ea1460dfa5f67b959d315b2cfd47983218f)

那么我们最后得到的目标时间应当就是本地时间+offset, 即$t3+\theta$， 而$\delta$就是RTT

2. 代码技术实现，采用用户界面和Client分离的方式。Client不依赖任何的库，编译为工件后作为一个库供用户界面使用。这样当我们需要修改一些Client中的计算算法时，不需要一起编译用户界面；在修改用户界面时，可以纯粹集中在用户交互体验方面，而不用关注后面的实现。

3. 时间精度

   原来的代码实现中，精度仅仅到秒为单位，但是时间戳提供的是64位，后32位即分秒数的位没有用上。我们解析了这部分的数据并将其也显示到了UI前端。当然实际上32位的分秒数可以比毫秒更精确，但是因为在Java虚拟机以及Windows非实时的操作系统上获取的时间比毫秒更高也意义不大，因此我们将最低的时间单位设置为1ms。

4. 在数据一致性方面，可以看到我们选取的源来源多样，但是展示出的偏差基本上都在相同的数量级，显示出较好地相关性。对于异常值，我们内置了简单的四分位距的算法来判断离群值，并在状态中显示此源是否可用。

5. 在可用性方面，除了解析除了Stratum、Poll、Precision等数据，我们还对于NTP的工作原理进一步剖析得到了更精确的时间，包括Local Clock Offset 和 Round Trip Time，帮助使用者进一步选择源。我们还根据源状态将源分为Pending\Ready\Success\Fail\Outlier等五个状态，分别代表未查看IP、已找到服务器IP、NTP请求成功、NTP请求失败、NTP请求返回时间为异常值。为了判别授时源的好坏，程序会自动计算jittery，即多次NTP请求结果offset的平均差别，反映的是服务器以及通信的稳定性。另外，我们采用了从配置文件source.csv中读取源的方法，更方便配置授时服务器源的选择。另外，当我们需要调试或是新增服务器外，还可以再UI中填写URL即可添加到源中。

6. 可以发现，有一些源不能通过DNS获取IP，猜测是因为DNS污染或是防火墙的原因；有一些源虽然可以通过DNS获取到IP，但是它们经常是不可用的，如美国地区和香港地区。我们ping工具也无法ping通，猜测可能是防火墙原因。还有一些是不是不能访问的服务器，猜测主要是因为NTP采用的是DatagramSocket通信，其基于的是UDP，可靠性难以保障，可能出现报文丢失的情况。